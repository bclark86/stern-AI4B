{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this term project, you will deploy Deep Learning models to build a classification model using RapidMiner to predict the sentiment of consumers towards US airlines based on their reviews expressed in the form of tweets. If you strongly prefer to use some other DL-based software/frameworks instead of RapidMiner, such as TensorFlow or PyTorch, let me know before starting the work. This is a group project, and you should work on it in the groups that you have formed already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# baseline algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# deep learning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.losses import mean_absolute_error as tf_mae\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# evaluation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_absolute_error as skl_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fetch Data\n",
    "\n",
    "The data is provided to you in two versions: \n",
    "\n",
    "1. The original version of the tweets (and their sentiments) is located at https://drive.google.com/file/d/1atyRH5Yz7TU-2ziyZknfd7ib2LLwYeuv/view?usp=sharing\n",
    "2. The preprocessed version of the tweets is located at https://drive.google.com/file/d/1c96crlNZr7XiF3-9lmZ1nEJaY3MHTTz5/view?usp=sharing, where text preprocessing and pre-training of the text embeddings of the tweets using autoencoders have already been done to make your life simpler. This preprocessed version contains the sentiments about the tweets in column 1 of the spreadsheet (either positive (1) or negative (0)) and the 8-dimenisonal pre-trained embeddings of the tweets (in columns 2 – 9 of the spreadsheet).\n",
    "\n",
    "I recommend that you use the preprocessed version of the tweets since it will save you a lot of preprocessing work to build these embeddings that is non-trivial. However, if you like challenges, you can do preprocessing and building the embeddings using autoencoders yourself and, therefore, work directly with the “raw” tweets. As a “reward” for this extra work, you will be awarded 10 extra points (the max score of this project is 100) if you preprocess tweets yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url of dataset\n",
    "google_drive_file_url = 'https://drivesds.google.com/file/d/1c96crlNZr7XiF3-9lmZ1nEJaY3MHTTz5/view?usp=sharing'\n",
    "\n",
    "def fetch_google_drive_csv(google_drive_file_url):\n",
    "\n",
    "    file_id = google_drive_file_url.split('/')[-2]\n",
    "    download_url = 'https://drive.google.com/uc?export=download&id=' + file_id\n",
    "    url = requests.get(download_url)\n",
    "    csv_raw = StringIO(url.text)\n",
    "    return pd.read_csv(csv_raw)\n",
    "\n",
    "data = fetch_google_drive_csv(google_drive_file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>dimension1</th>\n",
       "      <th>dimension2</th>\n",
       "      <th>dimension3</th>\n",
       "      <th>dimension4</th>\n",
       "      <th>dimension5</th>\n",
       "      <th>dimension6</th>\n",
       "      <th>dimension7</th>\n",
       "      <th>dimension8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.400418</td>\n",
       "      <td>0.293417</td>\n",
       "      <td>-0.572702</td>\n",
       "      <td>0.125659</td>\n",
       "      <td>0.471714</td>\n",
       "      <td>-0.034476</td>\n",
       "      <td>0.042176</td>\n",
       "      <td>-0.429317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.454608</td>\n",
       "      <td>-0.194998</td>\n",
       "      <td>-0.497063</td>\n",
       "      <td>0.242207</td>\n",
       "      <td>0.209621</td>\n",
       "      <td>0.064868</td>\n",
       "      <td>0.072154</td>\n",
       "      <td>0.629457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.515892</td>\n",
       "      <td>-0.120781</td>\n",
       "      <td>-0.106512</td>\n",
       "      <td>-0.260192</td>\n",
       "      <td>0.197666</td>\n",
       "      <td>-0.155029</td>\n",
       "      <td>-0.306803</td>\n",
       "      <td>0.694974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.047770</td>\n",
       "      <td>-0.230509</td>\n",
       "      <td>0.132355</td>\n",
       "      <td>0.174913</td>\n",
       "      <td>0.242040</td>\n",
       "      <td>-0.229259</td>\n",
       "      <td>-0.835945</td>\n",
       "      <td>0.294148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.574353</td>\n",
       "      <td>-0.132517</td>\n",
       "      <td>-0.091610</td>\n",
       "      <td>0.466463</td>\n",
       "      <td>0.510980</td>\n",
       "      <td>-0.338480</td>\n",
       "      <td>0.202040</td>\n",
       "      <td>-0.100443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  dimension1  dimension2  dimension3  dimension4  dimension5  \\\n",
       "0          1   -0.400418    0.293417   -0.572702    0.125659    0.471714   \n",
       "1          1   -0.454608   -0.194998   -0.497063    0.242207    0.209621   \n",
       "2          0   -0.515892   -0.120781   -0.106512   -0.260192    0.197666   \n",
       "3          1    0.047770   -0.230509    0.132355    0.174913    0.242040   \n",
       "4          0   -0.574353   -0.132517   -0.091610    0.466463    0.510980   \n",
       "\n",
       "   dimension6  dimension7  dimension8  \n",
       "0   -0.034476    0.042176   -0.429317  \n",
       "1    0.064868    0.072154    0.629457  \n",
       "2   -0.155029   -0.306803    0.694974  \n",
       "3   -0.229259   -0.835945    0.294148  \n",
       "4   -0.338480    0.202040   -0.100443  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55524 entries, 0 to 55523\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   sentiment   55524 non-null  int64  \n",
      " 1   dimension1  55524 non-null  float64\n",
      " 2   dimension2  55524 non-null  float64\n",
      " 3   dimension3  55524 non-null  float64\n",
      " 4   dimension4  55524 non-null  float64\n",
      " 5   dimension5  55524 non-null  float64\n",
      " 6   dimension6  55524 non-null  float64\n",
      " 7   dimension7  55524 non-null  float64\n",
      " 8   dimension8  55524 non-null  float64\n",
      "dtypes: float64(8), int64(1)\n",
      "memory usage: 3.8 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55524, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess Data\n",
    "\n",
    "Your task is to predict the score of the sentiment (positive or negative) between 0 and 1 based on the embeddings of the tweets specified in columns 2 – 9 of the pre-possessed spreadsheet (or the original tweets if you decided to work with the raw tweeting data). To evaluate the performance of your model, please split the dataset into the train set and the test set in the 0.8:0.2 ratio and use cross-validation to calculate the prediction performance.\n",
    "\n",
    "## 2.1 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'sentiment'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(target, axis=1), # predictors\n",
    "    data[target], # target\n",
    "    test_size=0.2,\n",
    "    random_state=90\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44419, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11105, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_splits = KFold(n_splits=5, random_state=90, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1: Train shape: (35535, 8), Test shape: (8884, 8)\n",
      "Fold #2: Train shape: (35535, 8), Test shape: (8884, 8)\n",
      "Fold #3: Train shape: (35535, 8), Test shape: (8884, 8)\n",
      "Fold #4: Train shape: (35535, 8), Test shape: (8884, 8)\n",
      "Fold #5: Train shape: (35536, 8), Test shape: (8883, 8)\n"
     ]
    }
   ],
   "source": [
    "fold_num = 1\n",
    "for train, test in cv_splits.split(X_train, y_train):\n",
    "    print(f'Fold #{fold_num}: Train shape: {X_train.iloc[train].shape}, Test shape: {X_train.iloc[test].shape}')\n",
    "    fold_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_score_keras(X_train, y_train, cv_spliter, model, batch_size, num_epochs):\n",
    "    \n",
    "    # model configuration\n",
    "    loss_function = tf_mae\n",
    "    optimizer = Adam()\n",
    "    callback = EarlyStopping(monitor='loss', patience=10)\n",
    "    \n",
    "    # cross-validation scores\n",
    "    cv_results = list()\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    for train, test in cv_spliter.split(X_train, y_train):\n",
    "\n",
    "        # compitle the model\n",
    "        model.compile(loss=loss_function,\n",
    "                      optimizer=optimizer)\n",
    "\n",
    "        # fit data to model\n",
    "        history = model.fit(X_train.iloc[train], y_train.iloc[train],\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=num_epochs,\n",
    "                            callbacks=[callback],\n",
    "                            verbose=0)\n",
    "\n",
    "        # generate generalization metrics\n",
    "        score = model.evaluate(X_train.iloc[test], y_train.iloc[test], verbose=0)\n",
    "        cv_results.append(score)\n",
    "        \n",
    "    return np.array(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural Networks\n",
    "\n",
    "You can use any neural network model you like for this classification task. In particular, you may start with a simple single fully connected network as a “baseline” and then try to use more complex models, including CNN and RNN based models, to achieve better performance results than this simple baseline model.  Your goal is to reach the mean absolute error of at least 0.48, which should not be too difficult. If you want to be more ambitious, you can try to reach the mean absolute error of 0.47 (medium difficulty), or even 0.46 (this is difficult). The higher accuracy you get, the more points you will be awarded. \n",
    "\n",
    "In addition to the simple NN baseline mentioned above, you should also build another basic baseline, such as a logistic regression model (similar to the one we used in the RapidMiner Lab done in the class) and compare the performance results of your DL-based model with that baseline. The expectation is that the more sophisticated DL-model should outperform simple baselines.\n",
    "\n",
    "## 3.1 Baseline Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression MAE: 0.4366 +/- 0.0041\n",
      "\n",
      "CPU times: user 551 ms, sys: 79 ms, total: 630 ms\n",
      "Wall time: 173 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_lr = LogisticRegression()\n",
    "cv_results_lr = cross_validate(model_lr, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv_splits)\n",
    "cv_lr_mu = -cv_results_lr['test_score'].mean()\n",
    "cv_lr_sd = cv_results_lr['test_score'].std()\n",
    "\n",
    "print(f'Logistic Regression MAE: {cv_lr_mu:.4f} +/- {cv_lr_sd:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MAE: 0.4186 +/- 0.0067\n",
      "\n",
      "CPU times: user 3min 21s, sys: 687 ms, total: 3min 22s\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_rf = RandomForestClassifier(n_estimators=500)\n",
    "cv_results_rf = cross_validate(model_rf, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv_splits)\n",
    "cv_rf_mu = -cv_results_rf['test_score'].mean()\n",
    "cv_rf_sd = cv_results_rf['test_score'].std()\n",
    "\n",
    "print(f'Random Forest MAE: {cv_rf_mu:.4f} +/- {cv_rf_sd:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simlpe NN Baseline Model #0 MAE: 0.4703 +/- 0.0046\n",
      "\n",
      "CPU times: user 27.7 s, sys: 5.04 s, total: 32.7 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# define the model architecture\n",
    "model_nn_0 = Sequential([\n",
    "    Dense(32, activation='relu', input_dim=8),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# cross-validation\n",
    "cv_results_nn_0 = cross_val_score_keras(\n",
    "    X_train, y_train,\n",
    "    cv_splits,\n",
    "    model_nn_0,\n",
    "    batch_size=50,\n",
    "    num_epochs=100\n",
    ")\n",
    "\n",
    "# summary statistics\n",
    "cv_nn_0_mu = cv_results_nn_0.mean()\n",
    "cv_nn_0_sd = cv_results_nn_0.std()\n",
    "print(f'Simlpe NN Baseline Model #0 MAE: {cv_nn_0_mu:.4f} +/- {cv_nn_0_sd:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Deep Learning\n",
    "\n",
    "### 4.2.1 DL Model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning Model #1 MAE: 0.4147 +/- 0.0044\n",
      "\n",
      "CPU times: user 2min 15s, sys: 42.5 s, total: 2min 57s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# define the model architecture\n",
    "model_nn_1 = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=8),\n",
    "    BatchNormalization(),\n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# cross-validation\n",
    "cv_results_nn_1 = cross_val_score_keras(\n",
    "    X_train, y_train,\n",
    "    cv_splits,\n",
    "    model_nn_1,\n",
    "    batch_size=50,\n",
    "    num_epochs=100\n",
    ")\n",
    "\n",
    "# summary statistics\n",
    "cv_nn_1_mu = cv_results_nn_1.mean()\n",
    "cv_nn_1_sd = cv_results_nn_1.std()\n",
    "print(f'Deep Learning Model #1 MAE: {cv_nn_1_mu:.4f} +/- {cv_nn_1_sd:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 DL Model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning Model #1 MAE: 0.4211 +/- 0.0053\n",
      "\n",
      "CPU times: user 2min 30s, sys: 45.6 s, total: 3min 15s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# define the model architecture\n",
    "model_nn_2 = Sequential([\n",
    "    Dense(32, activation='relu', input_dim=8),\n",
    "    BatchNormalization(),\n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# cross-validation\n",
    "cv_results_nn_2 = cross_val_score_keras(\n",
    "    X_train, y_train,\n",
    "    cv_splits,\n",
    "    model_nn_2,\n",
    "    batch_size=50,\n",
    "    num_epochs=100\n",
    ")\n",
    "\n",
    "# summary statistics\n",
    "cv_nn_2_mu = cv_results_nn_2.mean()\n",
    "cv_nn_2_sd = cv_results_nn_2.std()\n",
    "print(f'Deep Learning Model #1 MAE: {cv_nn_2_mu:.4f} +/- {cv_nn_2_sd:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
